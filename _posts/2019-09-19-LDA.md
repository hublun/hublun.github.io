---
layout: post
title: Latent Dirichlet Allocation (LDA)
subtitle: customer=document and SKU=word and Store=Vocabulary
cover-img: /assets/img/path.jpg
thumbnail-img: /assets/img/Dirichlet.png
share-img: /assets/img/path.jpg
tags: [nlp]
comments: true
---
___ 
## Latent Dirichlet Allocation (LDA)

by David Blei, Andrew Ng, and Michael Jordan

[Abstract](https://jmlr.csail.mit.edu/papers/v3/blei03a.html)


{: .box-error}
We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.

___ 

## Autoencoding Variational Inference For Topic Models (ProdLDA)

by Akash Srivastava, Charles Sutton

[Abstract](https://arxiv.org/abs/1703.01488)

{: .box-error}
Topic models are one of the most popular methods for learning representations of text, but a major challenge is that any change to the topic model requires mathematically deriving a new inference algorithm. A promising approach to address this problem is autoencoding variational Bayes (AEVB), but it has proven diffi- cult to apply to topic models in practice. We present what is to our knowledge the first effective AEVB based inference method for latent Dirichlet allocation (LDA), which we call Autoencoded Variational Inference For Topic Model (AVITM). This model tackles the problems caused for AEVB by the Dirichlet prior and by component collapsing. We find that AVITM matches traditional methods in accuracy with much better inference time. Indeed, because of the inference network, we find that it is unnecessary to pay the computational cost of running variational optimization on test data. Because AVITM is black box, it is readily applied to new topic models. As a dramatic illustration of this, we present a new topic model called ProdLDA, that replaces the mixture model in LDA with a product of experts. By changing only one line of code from LDA, we find that ProdLDA yields much more interpretable topics, even if LDA is trained via collapsed Gibbs sampling.

___
