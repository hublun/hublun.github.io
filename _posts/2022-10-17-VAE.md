---
layout: post
title: Variational Autoencoders (VAE)
subtitle: VAE marries graphical models and deep learning.
cover-img: /assets/img/path.jpg
thumbnail-img: /assets/img/vae.png
share-img: /assets/img/path.jpg
tags: [vae]
---

___

**An Introduction to Variational Autoencoders**

by Diederik P. Kingma; Max Welling

**Abstract**:


{: .box-warning}
In this monograph, the authors present an introduction to the framework of variational autoencoders (VAEs) that provides a principled method for jointly learning deep latent-variable models and corresponding inference models using stochastic gradient descent. The framework has a wide array of applications from generative modeling, semi-supervised learning to representation learning. The authors expand earlier work and provide the reader with the fine detail on the important topics giving deep insight into the subject for the expert and student alike. Written in a survey-like nature the text serves as a review for those wishing to quickly deepen their knowledge of the topic. An Introduction to Variational Autoencoders provides a quick summary for the reader of a topic that has become an important tool in modern-day deep learning techniques.


[DOI](http://ieeexplore.ieee.org/document/9051780)

___

**πVAE: a stochastic process prior for Bayesian deep learning with MCMC**

by Swapnil Mishra, Seth Flaxman, Tresnia Berah, Harrison Zhu, Mikko Pakkanen, Samir Bhatt

[**Abstract**](https://arxiv.org/abs/2002.06873v6)


{: .box-warning}
Stochastic processes provide a mathematically elegant way model complex data. In theory, they provide flexible priors over function classes that can encode a wide range of interesting assumptions. In practice, however, efficient inference by optimisation or marginalisation is difficult, a problem further exacerbated with big data and high dimensional input spaces. We propose a novel variational autoencoder (VAE) called the prior encoding variational autoencoder (πVAE). The πVAE is finitely exchangeable and Kolmogorov consistent, and thus is a continuous stochastic process. We use πVAE to learn low dimensional embeddings of function classes. We show that our framework can accurately learn expressive function classes such as Gaussian processes, but also properties of functions to enable statistical inference (such as the integral of a log Gaussian process). For popular tasks, such as spatial interpolation, πVAE achieves state-of-the-art performance both in terms of accuracy and computational efficiency. Perhaps most usefully, we demonstrate that the low dimensional independently distributed latent space representation learnt provides an elegant and scalable means of performing Bayesian inference for stochastic processes within probabilistic programming languages such as Stan.

___ 