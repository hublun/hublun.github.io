---
layout: post
title: TimeGPT-1
subtitle: Generative Pre-trained Foundation Models.
cover-img: /assets/img/path.jpg
thumbnail-img: /assets/img/gpt.png
share-img: /assets/img/path.jpg
tags: [gpt]
---

___
**[TimeGPT-1](https://arxiv.org/abs/2310.03589)**

by Azul Garza, Max Mergenthaler-Canseco

**Abstract**

In this paper, we introduce TimeGPT, the first foundation model for time series, capable of generating accurate predictions for diverse datasets not seen during training. We evaluate our pre-trained model against established statistical, machine learning, and deep learning methods, demonstrating that TimeGPT zero-shot inference excels in performance, efficiency, and simplicity. Our study provides compelling evidence that insights from other domains of artificial intelligence can be effectively applied to time series analysis. We conclude that large-scale time series models offer an exciting opportunity to democratize access to precise predictions and reduce uncertainty by leveraging the capabilities of contemporary advancements in deep learning.
___

**scGPT: Towards Building a Foundation Model for Single-Cell Multi-omics Using Generative AI**

by Haotian Cui, Chloe Wang,  Hassaan Maan,  Bo Wang

**Abstract**:


{: .box-warning}
Generative pre-trained models have achieved remarkable success in various domains such as natural language processing and computer vision. Specifically, the combination of large-scale diverse datasets and pre-trained transformers has emerged as a promising approach for developing foundation models. While texts are made up of words, cells can be characterized by genes. This analogy inspires us to explore the potential of foundation models for cell and gene biology. By leveraging the exponentially growing single-cell sequencing data, we present the first attempt to construct a single-cell foundation model through generative pre-training on over 10 million cells. We demonstrate that the generative pre-trained transformer, scGPT, effectively captures meaningful biological insights into genes and cells. Furthermore, the model can be readily finetuned to achieve state-of-the-art performance across a variety of downstream tasks, including multi-batch integration, multi-omic integration, cell-type annotation, genetic perturbation prediction, and gene network inference.

![IMG](https://www.biorxiv.org/content/biorxiv/early/2023/05/01/2023.04.30.538439/F1.large.jpg?width=800&height=600&carousel=1)

[Github Repo](https://github.com/bowang-lab/scGPT)

@article{cui2023scGPT,
title={scGPT: Towards Building a Foundation Model for Single-Cell Multi-omics Using Generative AI},
author={Cui, Haotian and Wang, Chloe and Maan, Hassaan and Pang, Kuan and Luo, Fengning and Wang, Bo},
journal={bioRxiv},
year={2023},
publisher={Cold Spring Harbor Laboratory}
}

____
